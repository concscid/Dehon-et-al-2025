######METAGENOMIC SEQUENCING: SHORT-READ ASSEMBLY, MAPPING AND MAG RECONSTRUCTION USING ANVI'O#####
###The workflow and code used, except blasts against specific databases, are all part of anvi'o (see the official website for more information): https://anvio.org/ 
###Before starting: put fastq pair-end read files in a folder named 00_RAW and a create a .txt file that contains sample names in the first column, the pathway to R1 files in the second column and the pathway to R2 files in the third column.
conda activate anvio-8

###Step 1: quality control of short-reads using the criteria described by Minoche et al.
mkdir 01_QC
iu-gen-configs samples.txt -o 01_QC
ls 01_QC/
Sample_01.ini
Sample_02.ini
for ini in 01_QC/*.ini; do iu-filter-quality-minoche $ini; done

###Step 2: metagenomic short-read assembly using MEGAHIT
cd 01_QC
cat *R1.fastq > R1.fastq
cat *R2.fastq > R2.fastq
gzip R*.fastq
megahit -1 R1.fastq.gz -2 R2.fastq.gz --min-contig-len 1000 -o /metals/DNA/megahit -t 16 --k-list 21,41,61,81,99
cd ../
cd megahit  
anvi-script-reformat-fasta final.contigs.fa -o contigs.fa --simplify-names --report-file rename.txt

####Step 3: mapping of short reads onto contigs using Bowtie2
cd ../
NUM_THREADS=16
names='D0_AB1_1 D0_AB1_2 D0_AB1_3 D0_ABM1 D0_ABM2 D0_ABM3 D0_Abx_1 D0_Abx_2 D0_Abx_3 D0_C1 D0_C2 D0_C3 D0_M1 D0_M2 D0_M3 D2_AB1_1 D2_AB1_2 D2_AB1_3 D2_ABM1 D2_ABM2 D2_ABM3 D2_Abx_1 D2_Abx_2 D2_Abx_3 D2_C1 D2_C2 D2_C3 D2_M1 D2_M2 D2_M3 D30_AB1_1 D30_AB1_2 D30_AB1_3 D30_ABM1 D30_ABM2 D30_ABM3 D30_Abx_1 D30_Abx_2 D30_Abx_3 D30_C1 D30_C2 D30_C3 D30_M1 D30_M2 D30_M3 D7_AB1_1 D7_AB1_2 D7_AB1_3 D7_ABM1 D7_ABM2 D7_ABM3 D7_Abx_1 D7_Abx_2 D7_Abx_3 D7_C1 D7_C2 D7_C3 D7_M1 D7_M2 D7_M3'
mkdir 04_MAPPING
bowtie2-build megahit/contigs.fa 04_MAPPING/contigs

for sample in $names
do
bowtie2 --threads $NUM_THREADS -x 04_MAPPING/contigs -1 metals/01_QC/${sample}-QUALITY_PASSED_R1.fastq -2 metals/01_QC/${sample}-QUALITY_PASSED_R2.fastq --no-unal -S 04_MAPPING/$sample.sam
samtools view -F 4 -bS 04_MAPPING/$sample.sam > 04_MAPPING/$sample-RAW.bam
anvi-init-bam 04_MAPPING/$sample-RAW.bam -o 04_MAPPING/$sample.bam
rm 04_MAPPING/$sample.sam 04_MAPPING/$sample-RAW.bam
done

###Step 4: create a contig database from the contig file
anvi-gen-contigs-database -f /metals/megahit/contigs.fa -o contigs.db -n 'Temp_MG'

#####Hidden Markov models: utilize multiple default bacterial single-copy core gene collections and identify hits among your genes to those collections using HMMER
anvi-run-hmms -c contigs.db --num-threads 16

####Taxonomic anotation of contigs using single copy genes
anvi-run-scg-taxonomy -c contigs.db


####Step 5: Create individual profiles for each sample
names='D0_AB1_1 D0_AB1_2 D0_AB1_3 D0_ABM1 D0_ABM2 D0_ABM3 D0_Abx_1 D0_Abx_2 D0_Abx_3 D0_C1 D0_C2 D0_C3 D0_M1 D0_M2 D0_M3 D2_AB1_1 D2_AB1_2 D2_AB1_3 D2_ABM1 D2_ABM2 D2_ABM3 D2_Abx_1 D2_Abx_2 D2_Abx_3 D2_C1 D2_C2 D2_C3 D2_M1 D2_M2 D2_M3 D30_AB1_1 D30_AB1_2 D30_AB1_3 D30_ABM1 D30_ABM2 D30_ABM3 D30_Abx_1 D30_Abx_2 D30_Abx_3 D30_C1 D30_C2 D30_C3 D30_M1 D30_M2 D30_M3 D7_AB1_1 D7_AB1_2 D7_AB1_3 D7_ABM1 D7_ABM2 D7_ABM3 D7_Abx_1 D7_Abx_2 D7_Abx_3 D7_C1 D7_C2 D7_C3 D7_M1 D7_M2 D7_M3'
mkdir 05_PROFILES
for sample in $names ; do anvi-profile -c contigs.db -i 04_MAPPING/$sample.bam --sample-name $sample --min-contig-length 1000 --output-dir 05_PROFILES/$sample ; done 

###Step 6: Merge profiles
mv contigs.db 05_PROFILES
cd 05_PROFILES
anvi-merge */PROFILE.db -o SAMPLES-MERGED -c contigs.db --enforce-hierarchical-clustering

###Contig visualization on anvi'o

anvi-interactive -p PROFILE.db -c contigs.db

###Step 7: Binning, MAG reconstruction and refinement. Binning is done manually based on differential. Bins are stored manually using the anvi'o interactive platform (for more information refer to the anvi'o website)
###Bin refinement on the interactive platform to satisfy quality criteria (completion >50% redundancy <10%) (-C name of the bin collection, -b name of the bin):
anvi-refine -p PROFILE.db -c contigs.db -C bins -b Bin_1
###Refined bins are stored manually based on differential coverage and sequence composition using the interactive platform and summarized using:
anvi-summarize -c contigs.db -p PROFILE.db -C bins
##The summary generated contains all the relevant information (MAG abundance, completion, redundancy, functional traits annotated with COG and pfam...) that was used in this study. Plots were obtained using GraphPad Prism 9

#####Step 8: completion and redundancy from MAGs - estimation with checkM2:

checkm2 predict --threads 16 --input MAGmetals/ -x fa --output-directory checkM2_MAGs/ --database_path checkM2_database/uniref100.KO.1.dmnd
###MAGs with <50% completion were discarded, even if anvi'o estimations were >50%


####Step 9: Annotation of genes of interest in the MAGs: 
#### 9.1 Diamond blast against specific databases (CARD for antibiotic resistance genes, BacMet for metal resistance genes)
###Example for Pseudomonas MAG and the CARD database, the same was done for all bins and databases, previously downloaded into the folder (databases created using diamond makedb)###
###Blast the contig sequences against the database and filter based on aminoacid identity (>60%) and alignment length (>100 aa) - choose best-hit

diamond blastx --db /databases/card.dmnd --query Pseudomonas-contigs.fa --evalue 0.00001 --out /card/card_contigsPseudomonas.txt -p 16 
awk '$3>=60 && $4>=100 {print;} card_contigsPseudomonas.txt > filtered_cardPseudomonas.txt
awk '!x[$1]++' filtered_cardPseudomonas.txt > besthit_cardPseudomonas.txt


########METAGENOMIC SEQUENCING: ARG AND MRG SCREENING IN NON-ASSEMBLED READS######
####Start from quality-filtered reads (step 1 in the previous code)
####Step 2: conversion to fasta
cd 01_QC
for function in *.fastq; do
fastx_toolkit/fastq_to_fasta -i $function -o $function.fasta -Q33
done
mkdir Fasta_File
mv *.fasta Fasta_File

###Step 3: concatenation of forward and reverse reads (without merging)
cd Fasta_File
ls *.fasta > sample_list.txt

while read ligne
do
	sed 's/\(.*\)\(_R\).*/\1/' > liste_names.txt
done < sample_list.txt

nl liste_names.txt | sort --key 2 --unique | cut --fields 2 > liste_unique_names.txt

while read name
do
	cat $name'_R1'.fastq.fasta  $name'_R2'.fastq.fasta > ./$name'concatenated'.fasta
done < liste_noms_uniques.txt

mkdir concatenated
mv concatenated* concatenated

####Step 4: blast against the CARD database (BacMet database was bacmet.dmnd)
for file in *.fasta
do
diamond blastx --db /databases/card.dmnd --query $file --evalue 0.00001 --out /metals/card_$file -p 16 
done

mkdir card_results
mv card* card_results
cd card_results

for file in *.fasta
do
awk '$3>=60 && $4>=100 {print;} $file > filtered_$file
done

mkdir filtered
mv filtered* filtered
cd filtered

for file in *.fasta
do
awk '!x[$1]++' $file > besthit_$file

####Step 5: abundance analysis using R

###Input: blast results (filtered) in .tsv format (one file per sample). Two columns: gene (gene names and accession numbers) and count (number of hits) Edit on excel as the example:
###"gene"	"count"
###"1"	"gb|AAA25550.1|ARO:3003105|dfrA3"	1
###"2"	"gb|AAA26793|ARO:3003748|oleC"	1

library(dplyr)
library(tidyverse)

###Create tables with unique gene columns per sample (do it individually for each sample):
setwd("CARD")
arg<-read.table(file ="D0_C1.txt", header = TRUE, sep = "\t")
arg2 <- arg %>% group_by(gene)
arg3 <- arg2 %>% summarise(n = n())
colnames(arg3)<-colnames(arg)
write.table(arg3, "D0_C1.tsv", sep ="\t")

setwd("../")
filenames<- list.files("CARD", pattern = "*.tsv", full.names=TRUE)
ldf<-lapply(filenames, read.delim, fill=TRUE, header=TRUE)
names(ldf)=str_remove(filenames,"^.*/")
ldf2<-imap_dfr(ldf, ~cbind(.x,sample=.y))
tab2<-pivot_wider(ldf2, names_from=sample, values_from=count, values_fill=0)
write.table(tab2, "arg_counts.txt")

###Gene counts were normalized per sequencing depth and plotted using GraphPad 9
